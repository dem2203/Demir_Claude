"""
DEMIR AI v8.0 - Enterprise Monitoring & Alerting System
COMPREHENSIVE SYSTEM MONITORING WITH PROMETHEUS, GRAFANA, ELASTICSEARCH
PROFESSIONAL ENTERPRISE IMPLEMENTATION
"""

import asyncio
import aiohttp
import psutil
import socket
import json
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple, Callable, Union
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from collections import deque, defaultdict
from enum import Enum
import prometheus_client
from prometheus_client import Counter, Gauge, Histogram, Summary, CollectorRegistry
from elasticsearch import AsyncElasticsearch
import structlog
import sentry_sdk
from sentry_sdk import capture_exception, capture_message
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import aiosmtplib
from jinja2 import Template
import yaml
import pickle
import hashlib
import traceback
import warnings
import os
import sys

# Structured logging setup
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()


# ====================== MONITORING ENUMS & STRUCTURES ======================

class AlertSeverity(Enum):
    """Alert severity levels"""
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"
    EMERGENCY = "emergency"


class MetricType(Enum):
    """Types of metrics"""
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    SUMMARY = "summary"


class HealthStatus(Enum):
    """System health status"""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    CRITICAL = "critical"


@dataclass
class Alert:
    """Alert data structure"""
    alert_id: str
    timestamp: datetime
    severity: AlertSeverity
    component: str
    title: str
    message: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    resolved: bool = False
    resolved_at: Optional[datetime] = None
    notification_sent: bool = False
    fingerprint: str = ""
    
    def __post_init__(self):
        if not self.fingerprint:
            self.fingerprint = self._generate_fingerprint()
    
    def _generate_fingerprint(self) -> str:
        """Generate unique fingerprint for deduplication"""
        data = f"{self.component}{self.title}{self.severity.value}"
        return hashlib.md5(data.encode()).hexdigest()


@dataclass
class Metric:
    """Metric data point"""
    name: str
    value: float
    timestamp: datetime
    labels: Dict[str, str] = field(default_factory=dict)
    metric_type: MetricType = MetricType.GAUGE
    unit: str = ""
    description: str = ""


@dataclass
class HealthCheck:
    """Health check result"""
    component: str
    status: HealthStatus
    timestamp: datetime
    latency_ms: float
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


# ====================== PROMETHEUS METRICS COLLECTOR ======================

class PrometheusMetricsCollector:
    """
    Prometheus Metrics Collection and Export
    PROFESSIONAL METRICS INSTRUMENTATION
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.registry = CollectorRegistry()
        self.port = config.get('prometheus_port', 9090)
        
        # Define metrics
        self._define_metrics()
        
        # Metrics buffer for batch processing
        self.metrics_buffer = deque(maxlen=10000)
        
        # Custom metrics storage
        self.custom_metrics: Dict[str, Any] = {}
        
        logger.info("PrometheusMetricsCollector initialized", port=self.port)
    
    def _define_metrics(self):
        """Define Prometheus metrics"""
        # Trading metrics
        self.trades_total = Counter(
            'trades_total',
            'Total number of trades executed',
            ['exchange', 'symbol', 'side'],
            registry=self.registry
        )
        
        self.trade_volume = Histogram(
            'trade_volume_usd',
            'Trade volume in USD',
            ['exchange', 'symbol'],
            buckets=[10, 50, 100, 500, 1000, 5000, 10000, 50000],
            registry=self.registry
        )
        
        self.position_pnl = Gauge(
            'position_pnl',
            'Current position P&L',
            ['symbol', 'side'],
            registry=self.registry
        )
        
        # System metrics
        self.cpu_usage = Gauge(
            'system_cpu_usage_percent',
            'System CPU usage percentage',
            registry=self.registry
        )
        
        self.memory_usage = Gauge(
            'system_memory_usage_bytes',
            'System memory usage in bytes',
            registry=self.registry
        )
        
        self.disk_usage = Gauge(
            'system_disk_usage_percent',
            'System disk usage percentage',
            ['mount_point'],
            registry=self.registry
        )
        
        # AI Model metrics
        self.model_predictions = Counter(
            'model_predictions_total',
            'Total model predictions',
            ['model_name', 'prediction_type'],
            registry=self.registry
        )
        
        self.model_accuracy = Gauge(
            'model_accuracy',
            'Model accuracy score',
            ['model_name'],
            registry=self.registry
        )
        
        self.model_latency = Histogram(
            'model_inference_latency_seconds',
            'Model inference latency',
            ['model_name'],
            buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0],
            registry=self.registry
        )
        
        # Network metrics
        self.websocket_connections = Gauge(
            'websocket_connections_active',
            'Active WebSocket connections',
            ['exchange'],
            registry=self.registry
        )
        
        self.api_requests = Counter(
            'api_requests_total',
            'Total API requests',
            ['method', 'endpoint', 'status'],
            registry=self.registry
        )
        
        self.api_latency = Histogram(
            'api_request_latency_seconds',
            'API request latency',
            ['method', 'endpoint'],
            registry=self.registry
        )
        
        # Database metrics
        self.db_connections = Gauge(
            'database_connections_active',
            'Active database connections',
            ['database'],
            registry=self.registry
        )
        
        self.db_query_duration = Histogram(
            'database_query_duration_seconds',
            'Database query duration',
            ['query_type', 'table'],
            registry=self.registry
        )
        
        # Custom business metrics
        self.daily_profit = Gauge(
            'daily_profit_usd',
            'Daily profit in USD',
            registry=self.registry
        )
        
        self.win_rate = Gauge(
            'trading_win_rate',
            'Trading win rate percentage',
            ['strategy'],
            registry=self.registry
        )
        
        self.sharpe_ratio = Gauge(
            'portfolio_sharpe_ratio',
            'Portfolio Sharpe ratio',
            registry=self.registry
        )
    
    def record_trade(self, exchange: str, symbol: str, side: str, volume: float):
        """Record trade metrics"""
        self.trades_total.labels(exchange=exchange, symbol=symbol, side=side).inc()
        self.trade_volume.labels(exchange=exchange, symbol=symbol).observe(volume)
    
    def update_position_pnl(self, symbol: str, side: str, pnl: float):
        """Update position P&L"""
        self.position_pnl.labels(symbol=symbol, side=side).set(pnl)
    
    def record_model_prediction(self, model_name: str, prediction_type: str, latency: float):
        """Record model prediction metrics"""
        self.model_predictions.labels(
            model_name=model_name,
            prediction_type=prediction_type
        ).inc()
        self.model_latency.labels(model_name=model_name).observe(latency)
    
    def update_model_accuracy(self, model_name: str, accuracy: float):
        """Update model accuracy"""
        self.model_accuracy.labels(model_name=model_name).set(accuracy)
    
    async def collect_system_metrics(self):
        """Collect system metrics"""
        # CPU usage
        cpu_percent = psutil.cpu_percent(interval=1)
        self.cpu_usage.set(cpu_percent)
        
        # Memory usage
        memory = psutil.virtual_memory()
        self.memory_usage.set(memory.used)
        
        # Disk usage
        for partition in psutil.disk_partitions():
            try:
                usage = psutil.disk_usage(partition.mountpoint)
                self.disk_usage.labels(mount_point=partition.mountpoint).set(usage.percent)
            except:
                pass
        
        # Network connections
        connections = len(psutil.net_connections())
        
        # Process-specific metrics
        process = psutil.Process()
        process_memory = process.memory_info().rss
        process_cpu = process.cpu_percent()
        
        return {
            'cpu_percent': cpu_percent,
            'memory_used': memory.used,
            'memory_percent': memory.percent,
            'process_memory': process_memory,
            'process_cpu': process_cpu,
            'network_connections': connections
        }
    
    def export_metrics(self) -> bytes:
        """Export metrics in Prometheus format"""
        return prometheus_client.generate_latest(self.registry)
    
    async def start_metrics_server(self):
        """Start Prometheus metrics HTTP server"""
        from aiohttp import web
        
        async def metrics_handler(request):
            metrics = self.export_metrics()
            return web.Response(body=metrics, content_type='text/plain')
        
        app = web.Application()
        app.router.add_get('/metrics', metrics_handler)
        
        runner = web.AppRunner(app)
        await runner.setup()
        site = web.TCPSite(runner, '0.0.0.0', self.port)
        await site.start()
        
        logger.info("Prometheus metrics server started", port=self.port)


# ====================== ELASTICSEARCH LOGGER ======================

class ElasticsearchLogger:
    """
    Elasticsearch Logging and Analysis
    CENTRALIZED LOG AGGREGATION
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.es_client = AsyncElasticsearch(
            hosts=config.get('hosts', ['localhost:9200']),
            http_auth=(config.get('username'), config.get('password')) if config.get('username') else None,
            use_ssl=config.get('use_ssl', False),
            verify_certs=config.get('verify_certs', False)
        )
        
        self.index_prefix = config.get('index_prefix', 'demirai')
        self.batch_size = config.get('batch_size', 100)
        self.log_buffer = []
        
        # Index templates
        self._setup_index_templates()
        
        logger.info("ElasticsearchLogger initialized")
    
    def _setup_index_templates(self):
        """Setup Elasticsearch index templates"""
        self.index_templates = {
            'logs': {
                'mappings': {
                    'properties': {
                        'timestamp': {'type': 'date'},
                        'level': {'type': 'keyword'},
                        'component': {'type': 'keyword'},
                        'message': {'type': 'text'},
                        'metadata': {'type': 'object'},
                        'trace_id': {'type': 'keyword'},
                        'span_id': {'type': 'keyword'}
                    }
                }
            },
            'metrics': {
                'mappings': {
                    'properties': {
                        'timestamp': {'type': 'date'},
                        'metric_name': {'type': 'keyword'},
                        'value': {'type': 'double'},
                        'labels': {'type': 'object'},
                        'aggregation_type': {'type': 'keyword'}
                    }
                }
            },
            'alerts': {
                'mappings': {
                    'properties': {
                        'timestamp': {'type': 'date'},
                        'severity': {'type': 'keyword'},
                        'component': {'type': 'keyword'},
                        'title': {'type': 'text'},
                        'message': {'type': 'text'},
                        'resolved': {'type': 'boolean'},
                        'fingerprint': {'type': 'keyword'}
                    }
                }
            }
        }
    
    async def log(self, level: str, component: str, message: str, **metadata):
        """Log message to Elasticsearch"""
        log_entry = {
            'timestamp': datetime.utcnow(),
            'level': level,
            'component': component,
            'message': message,
            'metadata': metadata,
            'host': socket.gethostname()
        }
        
        self.log_buffer.append(log_entry)
        
        # Flush if buffer is full
        if len(self.log_buffer) >= self.batch_size:
            await self.flush_logs()
    
    async def flush_logs(self):
        """Flush log buffer to Elasticsearch"""
        if not self.log_buffer:
            return
        
        index_name = f"{self.index_prefix}-logs-{datetime.utcnow():%Y.%m.%d}"
        
        # Bulk index
        actions = []
        for log_entry in self.log_buffer:
            actions.append({'index': {'_index': index_name}})
            actions.append(log_entry)
        
        try:
            await self.es_client.bulk(body=actions)
            self.log_buffer.clear()
        except Exception as e:
            logger.error("Failed to flush logs to Elasticsearch", error=str(e))
    
    async def search_logs(self, query: Dict[str, Any], size: int = 100) -> List[Dict]:
        """Search logs in Elasticsearch"""
        index_pattern = f"{self.index_prefix}-logs-*"
        
        response = await self.es_client.search(
            index=index_pattern,
            body=query,
            size=size
        )
        
        return [hit['_source'] for hit in response['hits']['hits']]
    
    async def aggregate_metrics(self, metric_name: str, time_range: str = '1h') -> Dict:
        """Aggregate metrics over time range"""
        index_pattern = f"{self.index_prefix}-metrics-*"
        
        query = {
            'query': {
                'bool': {
                    'must': [
                        {'term': {'metric_name': metric_name}},
                        {'range': {'timestamp': {'gte': f'now-{time_range}'}}}
                    ]
                }
            },
            'aggs': {
                'time_buckets': {
                    'date_histogram': {
                        'field': 'timestamp',
                        'interval': '5m'
                    },
                    'aggs': {
                        'avg_value': {'avg': {'field': 'value'}},
                        'max_value': {'max': {'field': 'value'}},
                        'min_value': {'min': {'field': 'value'}}
                    }
                }
            }
        }
        
        response = await self.es_client.search(
            index=index_pattern,
            body=query,
            size=0
        )
        
        return response['aggregations']
    
    async def close(self):
        """Close Elasticsearch connection"""
        await self.flush_logs()
        await self.es_client.close()


# ====================== ALERT MANAGER ======================

class AlertManager:
    """
    Alert Management and Notification System
    INTELLIGENT ALERT ROUTING AND SUPPRESSION
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.alerts: Dict[str, Alert] = {}
        self.alert_rules: List[Dict] = []
        self.notification_channels: Dict[str, Any] = {}
        
        # Alert deduplication
        self.alert_fingerprints: Dict[str, datetime] = {}
        self.dedup_window = timedelta(minutes=config.get('dedup_window_minutes', 5))
        
        # Alert suppression
        self.suppression_rules: List[Dict] = config.get('suppression_rules', [])
        self.maintenance_mode = False
        
        # Rate limiting
        self.rate_limits: Dict[str, deque] = defaultdict(lambda: deque(maxlen=100))
        
        # Initialize notification channels
        self._setup_notification_channels()
        
        # Alert statistics
        self.stats = {
            'total_alerts': 0,
            'active_alerts': 0,
            'suppressed_alerts': 0,
            'notifications_sent': 0
        }
        
        logger.info("AlertManager initialized")
    
    def _setup_notification_channels(self):
        """Setup notification channels"""
        # Email
        if 'email' in self.config:
            self.notification_channels['email'] = EmailNotifier(self.config['email'])
        
        # Slack
        if 'slack' in self.config:
            self.notification_channels['slack'] = SlackNotifier(self.config['slack'])
        
        # Telegram
        if 'telegram' in self.config:
            self.notification_channels['telegram'] = TelegramNotifier(self.config['telegram'])
        
        # PagerDuty
        if 'pagerduty' in self.config:
            self.notification_channels['pagerduty'] = PagerDutyNotifier(self.config['pagerduty'])
        
        # Webhook
        if 'webhook' in self.config:
            self.notification_channels['webhook'] = WebhookNotifier(self.config['webhook'])
    
    async def create_alert(self, 
                          severity: AlertSeverity,
                          component: str,
                          title: str,
                          message: str,
                          **metadata) -> Alert:
        """Create and process new alert"""
        # Create alert object
        alert = Alert(
            alert_id=self._generate_alert_id(),
            timestamp=datetime.utcnow(),
            severity=severity,
            component=component,
            title=title,
            message=message,
            metadata=metadata
        )
        
        # Check for deduplication
        if self._is_duplicate(alert):
            self.stats['suppressed_alerts'] += 1
            logger.debug("Alert deduplicated", fingerprint=alert.fingerprint)
            return None
        
        # Check suppression rules
        if self._should_suppress(alert):
            self.stats['suppressed_alerts'] += 1
            logger.debug("Alert suppressed", alert=alert.title)
            return None
        
        # Check rate limiting
        if not self._check_rate_limit(alert):
            self.stats['suppressed_alerts'] += 1
            logger.warning("Alert rate limited", component=component)
            return None
        
        # Store alert
        self.alerts[alert.alert_id] = alert
        self.alert_fingerprints[alert.fingerprint] = alert.timestamp
        
        # Update stats
        self.stats['total_alerts'] += 1
        self.stats['active_alerts'] = len([a for a in self.alerts.values() if not a.resolved])
        
        # Send notifications
        await self._send_notifications(alert)
        
        logger.info("Alert created", 
                   alert_id=alert.alert_id,
                   severity=severity.value,
                   component=component,
                   title=title)
        
        return alert
    
    def _generate_alert_id(self) -> str:
        """Generate unique alert ID"""
        return f"alert_{datetime.utcnow().timestamp()}_{os.urandom(4).hex()}"
    
    def _is_duplicate(self, alert: Alert) -> bool:
        """Check if alert is duplicate"""
        if alert.fingerprint in self.alert_fingerprints:
            last_seen = self.alert_fingerprints[alert.fingerprint]
            if alert.timestamp - last_seen < self.dedup_window:
                return True
        return False
    
    def _should_suppress(self, alert: Alert) -> bool:
        """Check if alert should be suppressed"""
        if self.maintenance_mode:
            return True
        
        for rule in self.suppression_rules:
            if self._match_suppression_rule(alert, rule):
                return True
        
        return False
    
    def _match_suppression_rule(self, alert: Alert, rule: Dict) -> bool:
        """Match alert against suppression rule"""
        # Check component match
        if 'component' in rule and rule['component'] != alert.component:
            return False
        
        # Check severity match
        if 'min_severity' in rule:
            min_severity = AlertSeverity[rule['min_severity'].upper()]
            if alert.severity.value < min_severity.value:
                return False
        
        # Check time window
        if 'time_window' in rule:
            start_time = datetime.strptime(rule['time_window']['start'], '%H:%M').time()
            end_time = datetime.strptime(rule['time_window']['end'], '%H:%M').time()
            current_time = datetime.utcnow().time()
            
            if start_time <= current_time <= end_time:
                return True
        
        return False
    
    def _check_rate_limit(self, alert: Alert) -> bool:
        """Check rate limiting for alerts"""
        key = f"{alert.component}:{alert.severity.value}"
        
        # Get rate limit configuration
        max_alerts = self.config.get('rate_limit', {}).get('max_alerts', 10)
        window_minutes = self.config.get('rate_limit', {}).get('window_minutes', 5)
        
        # Clean old entries
        cutoff_time = datetime.utcnow() - timedelta(minutes=window_minutes)
        self.rate_limits[key] = deque(
            [t for t in self.rate_limits[key] if t > cutoff_time],
            maxlen=max_alerts
        )
        
        # Check limit
        if len(self.rate_limits[key]) >= max_alerts:
            return False
        
        # Add current alert
        self.rate_limits[key].append(datetime.utcnow())
        return True
    
    async def _send_notifications(self, alert: Alert):
        """Send alert notifications"""
        # Determine channels based on severity
        channels = self._get_notification_channels(alert.severity)
        
        for channel_name in channels:
            if channel_name in self.notification_channels:
                try:
                    notifier = self.notification_channels[channel_name]
                    await notifier.send(alert)
                    alert.notification_sent = True
                    self.stats['notifications_sent'] += 1
                    logger.info("Notification sent", channel=channel_name, alert_id=alert.alert_id)
                except Exception as e:
                    logger.error("Failed to send notification", 
                               channel=channel_name, 
                               error=str(e))
    
    def _get_notification_channels(self, severity: AlertSeverity) -> List[str]:
        """Get notification channels based on severity"""
        severity_channels = {
            AlertSeverity.DEBUG: [],
            AlertSeverity.INFO: [],
            AlertSeverity.WARNING: ['slack'],
            AlertSeverity.ERROR: ['slack', 'email'],
            AlertSeverity.CRITICAL: ['slack', 'email', 'pagerduty'],
            AlertSeverity.EMERGENCY: ['slack', 'email', 'pagerduty', 'telegram']
        }
        return severity_channels.get(severity, [])
    
    async def resolve_alert(self, alert_id: str):
        """Resolve an alert"""
        if alert_id in self.alerts:
            alert = self.alerts[alert_id]
            alert.resolved = True
            alert.resolved_at = datetime.utcnow()
            
            self.stats['active_alerts'] = len([a for a in self.alerts.values() if not a.resolved])
            
            # Send resolution notification
            await self._send_resolution_notification(alert)
            
            logger.info("Alert resolved", alert_id=alert_id)
    
    async def _send_resolution_notification(self, alert: Alert):
        """Send alert resolution notification"""
        for channel_name, notifier in self.notification_channels.items():
            try:
                if hasattr(notifier, 'send_resolution'):
                    await notifier.send_resolution(alert)
            except Exception as e:
                logger.error("Failed to send resolution notification", 
                           channel=channel_name, 
                           error=str(e))
    
    def get_active_alerts(self) -> List[Alert]:
        """Get all active alerts"""
        return [alert for alert in self.alerts.values() if not alert.resolved]
    
    def get_alert_statistics(self) -> Dict:
        """Get alert statistics"""
        return self.stats.copy()


# ====================== NOTIFICATION CHANNELS ======================

class EmailNotifier:
    """Email notification channel"""
    
    def __init__(self, config: Dict):
        self.smtp_host = config['smtp_host']
        self.smtp_port = config['smtp_port']
        self.username = config['username']
        self.password = config['password']
        self.from_address = config['from_address']
        self.to_addresses = config['to_addresses']
        
    async def send(self, alert: Alert):
        """Send email notification"""
        message = MIMEMultipart('alternative')
        message['Subject'] = f"[{alert.severity.value.upper()}] {alert.title}"
        message['From'] = self.from_address
        message['To'] = ', '.join(self.to_addresses)
        
        # Create HTML content
        html_template = """
        <html>
        <body>
            <h2>Alert: {{ alert.title }}</h2>
            <p><strong>Severity:</strong> {{ alert.severity.value }}</p>
            <p><strong>Component:</strong> {{ alert.component }}</p>
            <p><strong>Time:</strong> {{ alert.timestamp }}</p>
            <p><strong>Message:</strong> {{ alert.message }}</p>
            {% if alert.metadata %}
            <h3>Additional Details:</h3>
            <ul>
            {% for key, value in alert.metadata.items() %}
                <li><strong>{{ key }}:</strong> {{ value }}</li>
            {% endfor %}
            </ul>
            {% endif %}
        </body>
        </html>
        """
        
        template = Template(html_template)
        html_content = template.render(alert=alert)
        
        html_part = MIMEText(html_content, 'html')
        message.attach(html_part)
        
        # Send email
        async with aiosmtplib.SMTP(hostname=self.smtp_host, port=self.smtp_port) as smtp:
            await smtp.login(self.username, self.password)
            await smtp.send_message(message)


class SlackNotifier:
    """Slack notification channel"""
    
    def __init__(self, config: Dict):
        self.webhook_url = config['webhook_url']
        self.channel = config.get('channel', '#alerts')
        
    async def send(self, alert: Alert):
        """Send Slack notification"""
        color_map = {
            AlertSeverity.DEBUG: '#808080',
            AlertSeverity.INFO: '#36a64f',
            AlertSeverity.WARNING: '#ff9900',
            AlertSeverity.ERROR: '#ff0000',
            AlertSeverity.CRITICAL: '#990000',
            AlertSeverity.EMERGENCY: '#660000'
        }
        
        payload = {
            'channel': self.channel,
            'attachments': [{
                'color': color_map.get(alert.severity, '#808080'),
                'title': alert.title,
                'text': alert.message,
                'fields': [
                    {'title': 'Severity', 'value': alert.severity.value, 'short': True},
                    {'title': 'Component', 'value': alert.component, 'short': True},
                    {'title': 'Time', 'value': str(alert.timestamp), 'short': False}
                ],
                'footer': 'DEMIR AI Alert System',
                'ts': int(alert.timestamp.timestamp())
            }]
        }
        
        async with aiohttp.ClientSession() as session:
            await session.post(self.webhook_url, json=payload)


class TelegramNotifier:
    """Telegram notification channel"""
    
    def __init__(self, config: Dict):
        self.bot_token = config['bot_token']
        self.chat_id = config['chat_id']
        self.api_url = f"https://api.telegram.org/bot{self.bot_token}"
        
    async def send(self, alert: Alert):
        """Send Telegram notification"""
        emoji_map = {
            AlertSeverity.DEBUG: 'üêõ',
            AlertSeverity.INFO: '‚ÑπÔ∏è',
            AlertSeverity.WARNING: '‚ö†Ô∏è',
            AlertSeverity.ERROR: '‚ùå',
            AlertSeverity.CRITICAL: 'üö®',
            AlertSeverity.EMERGENCY: 'üÜò'
        }
        
        message = f"""
{emoji_map.get(alert.severity, 'üì¢')} <b>{alert.title}</b>

<b>Severity:</b> {alert.severity.value}
<b>Component:</b> {alert.component}
<b>Time:</b> {alert.timestamp}

<b>Message:</b>
{alert.message}
"""
        
        payload = {
            'chat_id': self.chat_id,
            'text': message,
            'parse_mode': 'HTML'
        }
        
        async with aiohttp.ClientSession() as session:
            await session.post(f"{self.api_url}/sendMessage", json=payload)


class PagerDutyNotifier:
    """PagerDuty notification channel"""
    
    def __init__(self, config: Dict):
        self.integration_key = config['integration_key']
        self.api_url = 'https://events.pagerduty.com/v2/enqueue'
        
    async def send(self, alert: Alert):
        """Send PagerDuty notification"""
        severity_map = {
            AlertSeverity.DEBUG: 'info',
            AlertSeverity.INFO: 'info',
            AlertSeverity.WARNING: 'warning',
            AlertSeverity.ERROR: 'error',
            AlertSeverity.CRITICAL: 'critical',
            AlertSeverity.EMERGENCY: 'critical'
        }
        
        payload = {
            'routing_key': self.integration_key,
            'event_action': 'trigger',
            'payload': {
                'summary': alert.title,
                'source': alert.component,
                'severity': severity_map.get(alert.severity, 'error'),
                'timestamp': alert.timestamp.isoformat(),
                'custom_details': {
                    'message': alert.message,
                    **alert.metadata
                }
            }
        }
        
        async with aiohttp.ClientSession() as session:
            await session.post(self.api_url, json=payload)


class WebhookNotifier:
    """Generic webhook notification channel"""
    
    def __init__(self, config: Dict):
        self.url = config['url']
        self.headers = config.get('headers', {})
        
    async def send(self, alert: Alert):
        """Send webhook notification"""
        payload = asdict(alert)
        payload['timestamp'] = payload['timestamp'].isoformat()
        payload['severity'] = payload['severity'].value
        
        async with aiohttp.ClientSession() as session:
            await session.post(self.url, json=payload, headers=self.headers)


# ====================== HEALTH CHECKER ======================

class HealthChecker:
    """
    System Health Monitoring
    COMPREHENSIVE HEALTH CHECKS
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.health_checks: Dict[str, Callable] = {}
        self.health_status: Dict[str, HealthCheck] = {}
        self.check_interval = config.get('check_interval', 30)
        
        # Health history for trend analysis
        self.health_history = defaultdict(lambda: deque(maxlen=100))
        
        # Register default health checks
        self._register_default_checks()
        
        logger.info("HealthChecker initialized")
    
    def _register_default_checks(self):
        """Register default health checks"""
        self.register_check('database', self._check_database)
        self.register_check('redis', self._check_redis)
        self.register_check('api', self._check_api)
        self.register_check('websocket', self._check_websocket)
        self.register_check('disk_space', self._check_disk_space)
        self.register_check('memory', self._check_memory)
        self.register_check('trading_engine', self._check_trading_engine)
    
    def register_check(self, name: str, check_func: Callable):
        """Register a health check"""
        self.health_checks[name] = check_func
    
    async def run_health_checks(self) -> Dict[str, HealthCheck]:
        """Run all registered health checks"""
        results = {}
        
        for name, check_func in self.health_checks.items():
            start_time = datetime.utcnow()
            
            try:
                if asyncio.iscoroutinefunction(check_func):
                    status = await check_func()
                else:
                    status = check_func()
                
                latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
                
                health_check = HealthCheck(
                    component=name,
                    status=status,
                    timestamp=datetime.utcnow(),
                    latency_ms=latency_ms
                )
                
            except Exception as e:
                latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
                
                health_check = HealthCheck(
                    component=name,
                    status=HealthStatus.UNHEALTHY,
                    timestamp=datetime.utcnow(),
                    latency_ms=latency_ms,
                    error=str(e)
                )
                
                logger.error(f"Health check failed: {name}", error=str(e))
            
            results[name] = health_check
            self.health_status[name] = health_check
            self.health_history[name].append(health_check)
        
        return results
    
    async def _check_database(self) -> HealthStatus:
        """Check database health"""
        # Implementation would check actual database connection
        # For now, return healthy
        return HealthStatus.HEALTHY
    
    async def _check_redis(self) -> HealthStatus:
        """Check Redis health"""
        # Implementation would check actual Redis connection
        return HealthStatus.HEALTHY
    
    async def _check_api(self) -> HealthStatus:
        """Check API health"""
        # Implementation would check API endpoints
        return HealthStatus.HEALTHY
    
    async def _check_websocket(self) -> HealthStatus:
        """Check WebSocket connections"""
        # Implementation would check WebSocket connections
        return HealthStatus.HEALTHY
    
    def _check_disk_space(self) -> HealthStatus:
        """Check disk space"""
        usage = psutil.disk_usage('/')
        
        if usage.percent > 90:
            return HealthStatus.CRITICAL
        elif usage.percent > 80:
            return HealthStatus.UNHEALTHY
        elif usage.percent > 70:
            return HealthStatus.DEGRADED
        else:
            return HealthStatus.HEALTHY
    
    def _check_memory(self) -> HealthStatus:
        """Check memory usage"""
        memory = psutil.virtual_memory()
        
        if memory.percent > 90:
            return HealthStatus.CRITICAL
        elif memory.percent > 80:
            return HealthStatus.UNHEALTHY
        elif memory.percent > 70:
            return HealthStatus.DEGRADED
        else:
            return HealthStatus.HEALTHY
    
    async def _check_trading_engine(self) -> HealthStatus:
        """Check trading engine health"""
        # Implementation would check actual trading engine status
        return HealthStatus.HEALTHY
    
    def get_overall_health(self) -> HealthStatus:
        """Get overall system health"""
        if not self.health_status:
            return HealthStatus.UNHEALTHY
        
        statuses = [check.status for check in self.health_status.values()]
        
        if any(s == HealthStatus.CRITICAL for s in statuses):
            return HealthStatus.CRITICAL
        elif any(s == HealthStatus.UNHEALTHY for s in statuses):
            return HealthStatus.UNHEALTHY
        elif any(s == HealthStatus.DEGRADED for s in statuses):
            return HealthStatus.DEGRADED
        else:
            return HealthStatus.HEALTHY
    
    def get_health_trends(self, component: str, window: int = 10) -> Dict:
        """Get health trends for a component"""
        if component not in self.health_history:
            return {}
        
        history = list(self.health_history[component])[-window:]
        
        if not history:
            return {}
        
        return {
            'average_latency': np.mean([h.latency_ms for h in history]),
            'max_latency': max(h.latency_ms for h in history),
            'min_latency': min(h.latency_ms for h in history),
            'error_rate': sum(1 for h in history if h.error) / len(history),
            'status_distribution': {
                status.value: sum(1 for h in history if h.status == status) / len(history)
                for status in HealthStatus
            }
        }


# ====================== MONITORING ORCHESTRATOR ======================

class MonitoringOrchestrator:
    """
    Master Monitoring System Orchestrator
    COORDINATES ALL MONITORING COMPONENTS
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # Initialize components
        self.metrics_collector = PrometheusMetricsCollector(config.get('prometheus', {}))
        self.elasticsearch_logger = ElasticsearchLogger(config.get('elasticsearch', {}))
        self.alert_manager = AlertManager(config.get('alerts', {}))
        self.health_checker = HealthChecker(config.get('health', {}))
        
        # Sentry integration
        if 'sentry' in config:
            sentry_sdk.init(
                dsn=config['sentry']['dsn'],
                traces_sample_rate=config['sentry'].get('traces_sample_rate', 0.1),
                environment=config['sentry'].get('environment', 'production')
            )
        
        # Monitoring state
        self.running = False
        self.tasks = []
        
        # Performance metrics
        self.performance_metrics = {
            'monitoring_overhead': 0,
            'alerts_per_minute': 0,
            'metrics_per_second': 0
        }
        
        logger.info("MonitoringOrchestrator initialized")
    
    async def start(self):
        """Start monitoring system"""
        self.running = True
        
        # Start Prometheus metrics server
        asyncio.create_task(self.metrics_collector.start_metrics_server())
        
        # Start periodic tasks
        self.tasks.append(asyncio.create_task(self._collect_metrics_loop()))
        self.tasks.append(asyncio.create_task(self._health_check_loop()))
        self.tasks.append(asyncio.create_task(self._log_flush_loop()))
        self.tasks.append(asyncio.create_task(self._performance_monitor_loop()))
        
        logger.info("Monitoring system started")
    
    async def _collect_metrics_loop(self):
        """Periodic metrics collection"""
        while self.running:
            try:
                # Collect system metrics
                system_metrics = await self.metrics_collector.collect_system_metrics()
                
                # Log metrics to Elasticsearch
                await self.elasticsearch_logger.log(
                    'info',
                    'metrics',
                    'System metrics collected',
                    **system_metrics
                )
                
                # Check for alerts based on metrics
                await self._check_metric_alerts(system_metrics)
                
            except Exception as e:
                logger.error("Metrics collection error", error=str(e))
            
            await asyncio.sleep(10)  # Collect every 10 seconds
    
    async def _health_check_loop(self):
        """Periodic health checks"""
        while self.running:
            try:
                # Run health checks
                health_results = await self.health_checker.run_health_checks()
                
                # Process health results
                for component, health_check in health_results.items():
                    if health_check.status in [HealthStatus.UNHEALTHY, HealthStatus.CRITICAL]:
                        # Create alert for unhealthy component
                        severity = (AlertSeverity.CRITICAL 
                                  if health_check.status == HealthStatus.CRITICAL 
                                  else AlertSeverity.ERROR)
                        
                        await self.alert_manager.create_alert(
                            severity=severity,
                            component=component,
                            title=f"{component} is {health_check.status.value}",
                            message=health_check.error or "Component health check failed",
                            latency_ms=health_check.latency_ms
                        )
                
                # Update overall health metric
                overall_health = self.health_checker.get_overall_health()
                self.metrics_collector.custom_metrics['system_health'] = {
                    'value': 1 if overall_health == HealthStatus.HEALTHY else 0,
                    'status': overall_health.value
                }
                
            except Exception as e:
                logger.error("Health check error", error=str(e))
            
            await asyncio.sleep(self.health_checker.check_interval)
    
    async def _log_flush_loop(self):
        """Periodic log flushing"""
        while self.running:
            try:
                await self.elasticsearch_logger.flush_logs()
            except Exception as e:
                logger.error("Log flush error", error=str(e))
            
            await asyncio.sleep(5)  # Flush every 5 seconds
    
    async def _performance_monitor_loop(self):
        """Monitor monitoring system performance"""
        while self.running:
            try:
                # Calculate monitoring overhead
                process = psutil.Process()
                cpu_percent = process.cpu_percent()
                memory_mb = process.memory_info().rss / 1024 / 1024
                
                self.performance_metrics['monitoring_overhead'] = {
                    'cpu_percent': cpu_percent,
                    'memory_mb': memory_mb
                }
                
                # Calculate alert rate
                alert_stats = self.alert_manager.get_alert_statistics()
                self.performance_metrics['alerts_per_minute'] = (
                    alert_stats['total_alerts'] / max(1, (datetime.utcnow().minute + 1))
                )
                
                # Log performance metrics
                await self.elasticsearch_logger.log(
                    'debug',
                    'monitoring_performance',
                    'Monitoring system performance',
                    **self.performance_metrics
                )
                
            except Exception as e:
                logger.error("Performance monitoring error", error=str(e))
            
            await asyncio.sleep(60)  # Check every minute
    
    async def _check_metric_alerts(self, metrics: Dict):
        """Check metrics and create alerts if needed"""
        # CPU alert
        if metrics['cpu_percent'] > 90:
            await self.alert_manager.create_alert(
                severity=AlertSeverity.CRITICAL,
                component='system',
                title='High CPU Usage',
                message=f"CPU usage is at {metrics['cpu_percent']}%",
                cpu_percent=metrics['cpu_percent']
            )
        
        # Memory alert
        if metrics['memory_percent'] > 90:
            await self.alert_manager.create_alert(
                severity=AlertSeverity.CRITICAL,
                component='system',
                title='High Memory Usage',
                message=f"Memory usage is at {metrics['memory_percent']}%",
                memory_percent=metrics['memory_percent']
            )
    
    async def stop(self):
        """Stop monitoring system"""
        self.running = False
        
        # Cancel all tasks
        for task in self.tasks:
            task.cancel()
        
        await asyncio.gather(*self.tasks, return_exceptions=True)
        
        # Close connections
        await self.elasticsearch_logger.close()
        
        logger.info("Monitoring system stopped")
    
    def get_monitoring_status(self) -> Dict:
        """Get monitoring system status"""
        return {
            'running': self.running,
            'overall_health': self.health_checker.get_overall_health().value,
            'active_alerts': len(self.alert_manager.get_active_alerts()),
            'alert_stats': self.alert_manager.get_alert_statistics(),
            'performance': self.performance_metrics
        }
